# -*- coding: utf-8 -*-
"""Predict_Vine_Quality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oN_Nrfh4J04fdiUMzv3ES-aeRimz788s

**Table of Contents**

1. Introduction

2. Dataset Description

3. Exploratory Data Analysis (EDA)

4. Data Visualization

    - Quality Distribution

    - Features Histograms

    - Feature Importance

5. Modeling

6. Data Preprocessing

  - Deletion

  - Imputation

  - Data Correction

  - Feature Scaling

8. Comparison Table

9. Conclusion

10. References

# **Introduction**

The goal is to build a classification model to categorize wines based on their quality, using physicochemical information as features. To achieve this, two datasets are provided - one for red vinho verde wine and one for white vinho verde wine, both of which are produced in the north of Portugal. The dataset is modified by introducing 1% missing values. Modified data contains 6497 records and 13 columns.

# **Data Description**

**Data Features**:
   1.  fixed acidity
   2.  volatile acidity
   3.  citric acid
   4.  residual sugar
   5.  chlorides
   6.  free sulfur dioxide
   7.  total sulfur dioxide
   8.  density
   9.  pH
   10. sulphates
   11.  alcohol
   
**Target Variable**:
   12.  quality (score between 3 and 9)

# **Exploratory Data Analysis (EDA)**
"""

#import libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score,StratifiedKFold
from sklearn.metrics import accuracy_score,roc_auc_score,f1_score, classification_report
from sklearn.impute import KNNImputer
import warnings

warnings.filterwarnings("ignore")

#read csv file of red wine & store in pandas dataframe
df = pd.read_csv("/content/winequality-red.csv", sep =";")

#top 5 rows of data
df.head()

#summary of data
df.info()

#check missing values
df.isnull().sum()

#unique values of quality
df['quality'].unique()

#read csv file of white wine & store in pandas dataframe
dfw = pd.read_csv("/content/winequality-white.csv", sep =";")

#top 5 rows of dataframe
dfw.head()

#unique values in quality column
dfw.quality.unique()

#check missing values
dfw.isnull().sum()

#Concat red wine and white wine data
Data = pd.concat([df,dfw], ignore_index=True)

#Summary of new dataframe
Data.info()

#check missing values
Data.isnull().sum()

#unique values of quality
Data['quality'].unique()

#check duplicates
Data.duplicated().sum()

#drop duplicates rows
Data.drop_duplicates(inplace=True)

#number of records after removing duplicates
Data.shape[0]

#export data
Data.to_csv('WineData.csv', index = False)

#import data
WineData = pd.read_csv("/content/WineData.csv")

"""After concatenating the red and white wine data, we found 1179 duplicate records. However, we have removed these duplicate records and the remaining data contains 5318 records without any missing values. The quality column in the data has 7 unique values ranging from 3 to 9. The quality is ordinal, meaning that lower numbers indicate poorer quality. column i.e. 5, 6, 7, 4, 8, 3, 9. This is ordinal, here lower number means quality is poor.

# **Data Visualization**

## **Quality Distribution**
"""

#Countplot representing distribution of quality of wine
plt.figure(figsize=(7, 5))
#quality on x-axis and counts on y-axis
sns.countplot(data = WineData, x='quality', palette='Set3')
plt.title('Distribution of Wine Quality')
plt.xlabel('Quality of Wine')
plt.ylabel('Counts')
plt.show()

"""For classes 3, 4, 8, and 9, there are very few records available in the dataset, while classes 5,6 and 7 have a majority of the records. This indicates that the data is imbalanced.

## **Features Density Plots**
"""

#create a figure and an array of subplots
fig, axes = plt.subplots(3, 4, figsize=(12, 10))

#flatten the array of subplots for easy iteration
axes = axes.flatten()

#loop through each column and create a density plot in the corresponding subplot
for i, col in enumerate(WineData.columns):
    sns.kdeplot(data = WineData, x= WineData[col], fill=True, ax=axes[i])
    axes[i].set_title(col)

#adjust layout to prevent subplot overlap
plt.tight_layout()

#show the figure
plt.show()

"""Data is positively skewed in volatile acidity, citric acid, residual sugar, chlorides, free sulfulr dioxide, density, sulphates features. It would be better to look for outliers first to see if this skewness is because of outliers."""

#create a figure and an array of subplots
fig, axes = plt.subplots(3, 4, figsize=(12, 10))

#flatten the array of subplots for easy iteration
axes = axes.flatten()

#loop through each column and create a boxplot in the corresponding subplot
for i, col in enumerate(WineData.columns):
    sns.boxplot(x=WineData[col], ax=axes[i])
    axes[i].set_title(col)

#adjust layout to prevent subplot overlap
plt.tight_layout()

#show the figure
plt.show()

"""Its evident from above box plots there are outliers presents in features for which density graphs were skewed. Also in quality 3, 8, and 9 appeared as outliers.

## **Feature Importance**
"""

#correlation between quality and other remaining features
Cor =WineData.corr()['quality'][1:].drop('quality',axis=0)
plt.figure(figsize=(9,5))
sns.barplot(y=Cor.sort_values().index,x=Cor.sort_values().values)
plt.xlabel('Correlation values')
plt.text(-0.2,12, 'Figure: Correlation between quality and feature variables')
plt.show()

"""From above figure it can be seen quality is more related with alcohol concentration, and least correlated with density of the wine.

# **Modeling**

**Classification Task**

Build and train some ML model classifier and see which gives best results on original data. model which outperformed others will be choosed.
"""

#split data into train and test (90:10 ratio, shuffled)
train, test = train_test_split(WineData, test_size=0.10, shuffle=True, random_state=42)

x_train = train.drop('quality', axis=1)
y_train = train['quality']

#stratified K fold cross-validation
CrossVal = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

#list of different ML models
Models = [

     #max_iter increased for convergence
    ("Logistic Regression", LogisticRegression(random_state=42,max_iter=1000)),
    ("Decision Tree", DecisionTreeClassifier(random_state=42)),
    ("Random Forest", RandomForestClassifier(random_state=42)),
    ("SVM", SVC(random_state=42)),
    ("K-Nearest Neighbors", KNeighborsClassifier()),
    ("Naive Bayes", GaussianNB())
]

Results = []
ModelNames = []

#loop to train and evaluate models
for name, model in Models:
    scores = cross_val_score(model, x_train, y_train, cv=CrossVal, scoring="accuracy")
    Results.append(scores)
    ModelNames.append(name)
    AUC_score = np.mean(scores)
    print(f"{name}: Average AUC score = {AUC_score:.2f}")

#find the index of the best model based on average AUC score
ModelIndex = np.argmax([np.mean(scores) for scores in Results])
BestModel = Models[ModelIndex][1]
BestModelName = ModelNames[ModelIndex]
print(f"\nBest Model: {BestModelName}\nWith Average AUC score: {np.mean(Results[ModelIndex]):.2f}")

"""Since data is imbalanced, models are assessed using the Area Under the Receiver Operating Characteristic Curve (AUC) metric. Random Forest outperformed other classifiers with the highest average AUC score of 0.57. Logistic Regression followed with an AUC of 0.53, while Decision Tree, Naive Bayes, K-Nearest Neighbors, and SVM exhibited lower AUC scores ranging from 0.43 to 0.45. Thus, **Random Forest Classifier** is best model for this task.

**Regression Task**
"""

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
import numpy as np

#K Fold cross-validation (for regression, StratifiedKFold is not suitable)
CrossVal = KFold(n_splits=5, shuffle=True, random_state=42)

#list of different ML models for regression
Models = [
    ("Linear Regression", LinearRegression()),
    ("Decision Tree Regressor", DecisionTreeRegressor(random_state=42)),
    ("Random Forest Regressor", RandomForestRegressor(random_state=42)),
    ("SVR", SVR()),
    ("K-Nearest Neighbors Regressor", KNeighborsRegressor())
]

Results = []
ModelNames = []

#loop to train and evaluate regression models
for name, model in Models:
    scores = cross_val_score(model, x_train, y_train, cv=CrossVal, scoring="neg_mean_squared_error")

    #negate the scores to make them positive for easier interpretation
    scores = -scores
    Results.append(scores)
    ModelNames.append(name)

    #calculate Root Mean Squared Error
    RMSE_score = np.sqrt(np.mean(scores))
    print(f"{name}: Average RMSE score = {RMSE_score:.2f}")

#find the index of the best model based on average RMSE score
ModelIndex = np.argmin([np.mean(scores) for scores in Results])
BestModel = Models[ModelIndex][1]
BestModelName = ModelNames[ModelIndex]
print(f"\nBest Model: {BestModelName}\nWith Average RMSE score: {np.sqrt(np.mean(Results[ModelIndex])):.2f}")

"""The regression models were evaluated based on Root Mean Squared Error (RMSE) scores. Random Forest Regressor outperformed other models with the lowest average RMSE of 0.70, indicating more accurate predictions. Linear Regression and SVR followed with scores of 0.74 and 0.81, respectively. Average RMSE of 0.70 for the **Random Forest Regressor** signifies precise predictions, making it the best-performing model on given task."""

#Building Random Forest Classifier and training data

def RFbuild_train(train , test):

  x_train = train.drop('quality', axis=1)
  x_test = test.drop('quality', axis=1)
  y_train = train['quality']
  y_test =  test['quality']

  # Create a random forest tree classifier
  Rclf = RandomForestClassifier(max_depth=20, min_samples_leaf=1, min_samples_split=3, n_estimators=100)

  # Train the classifier on the training data
  Rclf.fit(x_train, y_train)

  # Make predictions on the testing data
  pred = Rclf.predict(x_test)

  return x_test, y_test, pred

"""# **Data Preprocessing**"""

#load data in dataframe
WineData = pd.read_csv('/content/WineData.csv')

"""Since there are no missing values we will keep 10% of data for testing and  remove 1% values randomly from remaining data and then work on this dataset."""

#Train-Test Split
Train, Test = train_test_split(WineData, test_size= 0.10, train_size= 0.90, shuffle = True)

#shape of test dataset
Test.shape

#shape of training dataset
Train.shape

#check missing values in training data
Train.isnull().sum()

#introduce missing values in train data except 'quality'
for col in Train.columns:
    if col != 'quality':
        Train.loc[Train.sample(frac=0.01).index, col] = np.nan

#check missing values in training data
Train.isnull().sum()

#check missing values in testing data
Test.isnull().sum()

"""**Exporting Data**"""

#export modified traing and test data
Train.to_csv('Training.csv', index = False)
Test.to_csv('Test.csv', index = False)

"""We will perform data preprocessing techniqies on this dataset

**Loading Data**
"""

TrainData = pd.read_csv('/content/Training.csv')
TestData = pd.read_csv('/content/Test.csv')

"""**Treating Missing Values**

## **Experiment 1**: Listwise Deletion
"""

#list to store AUC score
F1Score = []
Exp = []
Accuracy = []
Description = []

#drop rows which contain null values
Train1 = TrainData.dropna( how = 'any')

#check missing values
Train1.isnull().sum()

#check rows of data
print(f'Number of records before dropping missing value: {TrainData.shape[0]} \nNumber of records after dropping missing value {Train1.shape[0]}')

#call function to buil and train model, input is train and test data set, and retunrs x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train1,TestData)

# Evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 1')
Description.append('ListWise Deletion')

# Display evaluation metrics
print(f"Model Evaluation - EXP 1:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""## **Experiment 2**: Variable Deletion"""

Train2 = Train1.copy()

#correlation between quality and other remaining features
Cor =Train2.corr()['quality'][1:].drop('quality',axis=0)
plt.figure(figsize=(9,5))
sns.barplot(y=Cor.sort_values().index,x=Cor.sort_values().values)
plt.xlabel('Correlation values')
plt.text(-0.2,12, 'Figure: Correlation between quality and feature variables')
plt.show()

"""From figure it is observed  Total sulphur oxide, residual sugar have weak negative correlation. Drop these columns and see how it impacts results."""

Train2.columns

#drop rows which contain null values
Train2.drop(['total sulfur dioxide','residual sugar','free sulfur dioxide'], axis =1, inplace = True)
Test = TestData.drop(['total sulfur dioxide','residual sugar','free sulfur dioxide'], axis =1)

#check missing values
Train2.isnull().sum()

#check rows of data
print(f'Shape of Training dataset :\nBefore dropping variables: {Train1.shape} \nAfter dropping variables:{Train2.shape}')

print(f'\nShape of Test dataset :\nBefore dropping variables: {TestData.shape} \nAfter dropping variables:{Test.shape}')

#call function to buil and train model, input is train and test data set, and retunrs x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train2,Test)

# Evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 2')
Description.append('Variable Deletion')

#display evaluation metrics
print(f"Model Evaluation - EXP 2:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""## **Experiment 3**: Mean Substitution"""

Train3 = TrainData.copy()

#check missing values
print(f'Missing Values in dataset: {Train3.isnull().sum().sum()}')

#fill missing values with mean values
Train3.fillna(Train3.mean(), inplace=True)

#check missing values
print(f'Missing Values in dataset: {Train3.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and retunrs x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train3,TestData)

# Evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 3')
Description.append('Mean Substitution')

#display evaluation metrics
print(f"Model Evaluation - EXP 3:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""## **Experiment 4**: Median Substitution"""

Train4 = TrainData.copy()

#check missing values
print(f'Missing Values in dataset: {Train4.isnull().sum().sum()}')

#fill missing values with median
for column in Train4.columns:
    median_value = Train4[column].median()
    Train4[column].fillna(median_value, inplace=True)

#check missing values
print(f'Missing Values in dataset: {Train4.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and retunrs x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train4,TestData)

# Evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 4')
Description.append('Median Substitution')

#display evaluation metrics
print(f"Model Evaluation - EXP 4:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""## **Experiment 5**: Frequency Substitution"""

Train5 = TrainData.copy()

#check missing values
print(f'Missing Values in dataset: {Train5.isnull().sum().sum()}')

#fill missing values with mode
for column in Train5.columns:
    mode_value = Train5[column].mode().iloc[0]
    Train5[column].fillna(mode_value, inplace=True)

#check missing values
print(f'Missing Values in dataset: {Train5.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and retunrs x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train5,TestData)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 5')
Description.append('Frequency Substitution')

#display evaluation metrics
print(f"Model Evaluation - EXP 5:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""## **Experiment 6:** KNN Imputer"""

from sklearn.impute import KNNImputer
import pandas as pd
import numpy as np

Train6 = TrainData.copy()

#check missing values
print(f'Total missing values before KNN Imputation: {Train6.isnull().sum().sum()}')

#create KNNImputer instance
knn_imputer = KNNImputer(n_neighbors=5)

#impute missing values using KNN
ImputedData = knn_imputer.fit_transform(Train6)

#convert the result back to a DataFrame
Train6 = pd.DataFrame(ImputedData, columns=Train6.columns)

#check missing values
print(f'\nTotal missing values After KNN Imputation: {Train6.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and retunrs x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train6,TestData)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 6')
Description.append('KNN Imputer')

#display evaluation metrics
print(f"Model Evaluation - EXP 6:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""**Data Correction**

## **Experiment 7:** Nearest Neighbor Outliers Detection
"""

from sklearn.neighbors import LocalOutlierFactor
import pandas as pd
import numpy as np

Train7 = Train5.copy()

# Create a DataFrame with only the columns to check for outliers
data_to_check = Train7.drop('quality',axis = 1)

columns_to_check = data_to_check.columns

#specify the contamination parameter, which is the proportion of outliers in the data
contamination = 0.03

#create the LocalOutlierFactor model
lof_model = LocalOutlierFactor(contamination=contamination)

#fit the model and predict outliers
outlier_predictions = lof_model.fit_predict(data_to_check)

#identify rows with outliers
outlier_indices = np.where(outlier_predictions == -1)[0]

#drop rows with outliers from the original DataFrame
Train7 = Train7.drop(index=outlier_indices)

print(len(outlier_indices))

Train6.shape

Train7.shape

#check missing values
print(f'Missing Values in dataset: {Train7.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and retunrs x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train7,TestData)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 7')
Description.append('Nearest Neighbors Outliers Detection')

#display evaluation metrics
print(f"Model Evaluation - EXP 7:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""## **Experiment 8:** Isolation Forest Outliers Detection"""

from sklearn.ensemble import IsolationForest
import pandas as pd
import numpy as np

Train8 = Train5.copy()

# Create a DataFrame with only the columns to check for outliers
data_to_check = Train8.drop('quality',axis = 1)

columns_to_check = data_to_check.columns

#specify the contamination parameter, which is the proportion of outliers in the data
contamination = 0.03

#create the IsolationForest model
isolation_forest_model = IsolationForest(contamination=contamination, random_state=42)

#fit the model and predict outliers
OutlierPredictions = isolation_forest_model.fit_predict(data_to_check)

# Identify rows with outliers
OutlierIndices = np.where(OutlierPredictions == -1)[0]

# Drop rows with outliers from the original DataFrame
Train8 = Train8.drop(index=OutlierIndices)

print(len(OutlierIndices))

Train5.shape

Train8.shape

#check missing values
print(f'Missing Values in dataset: {Train8.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and retunrs x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train8,TestData)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 8')
Description.append('Isolation Forest Outliers Detection')

# Display evaluation metrics
print(f"Model Evaluation - EXP 8:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""**Feature Scaling**

## **Experiment 9**: Min-Max Scaling
"""

Train9 = Train5.copy()

transposed_stats = Train9.describe().transpose()
transposed_stats

"""Columns like 'residual sugar', 'free sulfur dioxide', and 'total sulfur dioxide' have a wide range of values. Scaling these columns might help ensure that each feature contributes more equally to the learning process."""

from sklearn.preprocessing import MinMaxScaler


columns_to_scale = [ 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide']

#create a MinMaxScaler
scaler = MinMaxScaler()

#apply Min-Max scaling on selected columns
Train9[columns_to_scale] = scaler.fit_transform(Train9[columns_to_scale])

# Apply same preprocessing techniques on test dataset
Test9 = TestData.copy()
Test9[columns_to_scale] = scaler.fit_transform(Test9[columns_to_scale])

transposed_stats = Train9.describe().transpose()
transposed_stats

#check missing values
print(f'Missing Values in dataset: {Train9.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and retunrs x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train9,TestData)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 9')
Description.append('Min-Max Scaling')

#display evaluation metrics
print(f"Model Evaluation - EXP 9:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""## **Experiment 10**: Min-Max Normalization on Instances"""

Train10 = Train5.copy()

transposed_stats = Train10.describe().transpose()
transposed_stats

"""Columns like 'residual sugar', 'free sulfur dioxide', and 'total sulfur dioxide' have a wide range of values. Normalizing these columns might help ensure that each feature contributes more equally to the learning process."""

from sklearn.preprocessing import Normalizer

#list of columns to normalize
columns_to_normalize = ['residual sugar', 'free sulfur dioxide', 'total sulfur dioxide']

#create a Normalizer
normalizer = Normalizer()

#fit the normalizer on training data and apply row-wise normalization on selected columns
Train10[columns_to_normalize] = normalizer.fit_transform(Train10[columns_to_normalize])

#apply same preprocessing techniqyes on test dataset
Test10 = TestData.copy()

#apply the same normalizer to transform the test dataset
Test10[columns_to_normalize] = normalizer.transform(Test10[columns_to_normalize])

transposed_stats = Train10.describe().transpose()
transposed_stats

#check missing values
print(f'Missing Values in dataset: {Train10.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and returns x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train10,Test10)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 10')
Description.append('Min-Max Normalization on Instances')

#display evaluation metrics
print(f"Model Evaluation - EXP 10:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

print("Classification Report:")
print(classification_report(y_test, pred))

"""## **Experiment 11:** Robust Scaling"""

import matplotlib.pyplot as plt
import seaborn as sns

#create a figure and an array of subplots
fig, axes = plt.subplots(3, 4, figsize=(12, 10))

#flatten the array of subplots for easy iteration
axes = axes.flatten()

#loop through each column and create a boxplot in the corresponding subplot
for i, col in enumerate(TrainData.columns):
    sns.boxplot(x=TrainData[col], ax=axes[i])
    axes[i].set_title(col)

#adjust layout to prevent subplot overlap
plt.tight_layout()

#show the figure
plt.show()

from sklearn.preprocessing import RobustScaler

Train11 = Train5.copy()

#list of columns to scale
columns_to_scale = (Train11.columns).difference(['alcohol','quality'])

#create a RobustScaler
robust_scaler = RobustScaler()

#fit the scaler on training data and apply robust scaling on selected columns
Train11[columns_to_scale] = robust_scaler.fit_transform(Train11[columns_to_scale])

Test11= TestData.copy()

#apply the same scaler to transform the test dataset
Test11[columns_to_scale] = robust_scaler.transform(Test11[columns_to_scale])

#check missing values
print(f'Missing Values in dataset: {Train11.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and returns x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train11,Test11)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 11')
Description.append('Robust Scaling')

#display evaluation metrics
print(f"Model Evaluation - EXP 11:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

"""## **Experiment 12:** Maximum Absolute Scaling on Columns"""

from sklearn.preprocessing import MaxAbsScaler

Train12 = Train5.copy()

columns_to_scale = ['residual sugar', 'free sulfur dioxide', 'total sulfur dioxide']

#create a MaxAbsScaler
scaler = MaxAbsScaler()

#apply MaxAbs Scaling to the specified columns
Train12[columns_to_scale] = scaler.fit_transform(Train12[columns_to_scale])

#apply same techniques on test set

Test12 = TestData.copy()

#apply MaxAbs Scaling to the specified columns
Test12[columns_to_scale] = scaler.fit_transform(Test12[columns_to_scale])

#check missing values
print(f'Missing Values in dataset: {Train12.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and returns x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train12,Test12)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 12')
Description.append('Maximum Absoulte Scaling on Columns')

#display evaluation metrics
print(f"Model Evaluation - EXP 12:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

"""## **Experiment 13:** Standardized Scaling"""

from sklearn.preprocessing import StandardScaler

Train13 = Train5.copy()
Test13 = TestData.copy()

#list of columns to scale using Standard Scaler
columns_to_scale = (Train11.columns).difference(['alcohol','quality'])

#crreate a MaxAbsScaler
scaler = StandardScaler()

#apply Standard Scaling to the specified columns
Train13[columns_to_scale] = scaler.fit_transform(Train13[columns_to_scale])

#apply Standard Scaling on test data
Test13[columns_to_scale] = scaler.fit_transform(Test13[columns_to_scale])

#check missing values
print(f'Missing Values in dataset: {Train13.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and returns x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train13,Test13)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 13')
Description.append('Standardize Scaling')

#display evaluation metrics
print(f"Model Evaluation - EXP 13:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

"""## **Experiment 14:** Log Transformation"""

Train14 = Train5.copy()

columns_to_transform = ['residual sugar', 'free sulfur dioxide', 'total sulfur dioxide']

#apply log transformation to multiple columns
for column in columns_to_transform:
    Train14['Log_' + column] = np.log1p(Train14[column])

Test14 = TestData.copy()
#apply log transformation to multiple columns
for column in columns_to_transform:
    Test14['Log_' + column] = np.log1p(Test14[column])

Test14.columns

#check missing values
print(f'Missing Values in dataset: {Train14.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and returns x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train14,Test14)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 14')
Description.append('Log Transformation')

#display evaluation metrics
print(f"Model Evaluation - EXP 14:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

"""## **Experiment 15:** Box-Cox Transformation"""

from sklearn.preprocessing import power_transform

Train15 = Train5.copy()

#list of columns to scale using Standard Scaler
columns_to_transform = (Train15.columns).difference(['alcohol','quality'])

constant = 0.0001
#apply Box-Cox Transformation to the specified columns
Train15[columns_to_transform] = power_transform(Train15[columns_to_transform]+constant,method='box-cox')

#apply same techniques on test set

Test15 = TestData.copy()

#apply Box-Cox Transformation to the specified columns
Test15[columns_to_transform] = power_transform(Test15[columns_to_transform]+constant,method='box-cox')

#check missing values
print(f'Missing Values in dataset: {Train15.isnull().sum().sum()}')

#call function to buil and train model, input is train and test data set, and returns x_test,y_test, pred
x_test, y_test, pred = RFbuild_train(Train15,Test15)

#evaluate the performance of the classifier

#calculate accuracy
accuracy = accuracy_score(y_test, pred)
Accuracy.append(round(accuracy, 5))

#calculate F1 Score
#'weighted' takes class imbalance into account
f1 = f1_score(y_test, pred, average='weighted')
F1Score.append(round(f1, 5))

Exp.append('EXP 15')
Description.append('Box-Cox Transformation')

#display evaluation metrics
print(f"Model Evaluation - EXP 15:")
print(f"Accuracy: {accuracy:.5f}")
print(f"Weighted F1 Score: {f1:.5f}")

"""# **Comparison Table**"""

data = {
    'Experiments': Exp,
    'Description': Description,
    'Accuracy': Accuracy,
    'F1-Score': F1Score,
    'Improvement': ["-", round(F1Score[1] - F1Score[0], 5), round(F1Score[2] - F1Score[0], 5), round(F1Score[3] - F1Score[0], 5),
                    round(F1Score[4] - F1Score[0], 5), round(F1Score[5] - F1Score[4], 5), round(F1Score[6] - F1Score[4], 5),
                    round(F1Score[7] - F1Score[4], 5), round(F1Score[8] - F1Score[4], 5), round(F1Score[9] - F1Score[4], 5),
                    round(F1Score[10] - F1Score[4], 5), round(F1Score[11] - F1Score[4], 5), round(F1Score[12] - F1Score[4], 5),
                    round(F1Score[13] - F1Score[4], 5), round(F1Score[14] - F1Score[4], 5)],
     'Comment': ['Baseline 1','-','-','-','Baseline 2','-','-','-','-','-','-','-','-','-','-']
}

#create DataFrame
df = pd.DataFrame(data)

#display DataFrame
print(df)

from tabulate import tabulate

def format_improvement(value):
    if isinstance(value, str):
        return value  #no formatting for the first string value

    #red for positive, Blue for negative
    color = '\033[91m' if value > 0 else '\033[94m'

    #up arrow for positive, Down arrow for negative
    arrow = '↑' if value > 0 else '↓'
    return f'{color}{arrow} {value:.5f}\033[0m'

#apply the formatting function to the 'Improvement' column
df['Improvement'] = df['Improvement'].apply(format_improvement)

#display the formatted DataFrame
table = tabulate(df, headers='keys', tablefmt='grid', showindex=False)

"""# **Comparison Table**"""

print(table)

"""# **Conclusion**

# **References**

1 P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.
"""